{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNuIT+Doz4/LpxjwglAqxDq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adhilnajeeb7/ICT/blob/main/movie_sentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5E6IxgTCwQBt",
        "outputId": "72083181-3db4-408b-818d-afc96606dc1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your movie review: I'll get right to the point. This is the 2nd worst Godzilla flic since Godzilla 1985. As usual, Legendary manages to make even the charactors/actors with the greatest potential and in their usual fashion, turns them into cardboad. Not even the semi-clever quips work to give these characters a heart. Kaylee Hottle is very talented and the poor script manages to snuff out her true potential. Same for Dan Stevens who has the potential to do great things but again, the script is so trite he is lost in the murk. The other cast members did a phone-in \"performance\" that a high schooler could have done.....maybe even better. As far as the CGI: over utilized and a quality that pales in comparison to Minus 1. Kong and Mini-Kong had rediculous facial expressions and the fight scenes between them looked like WWW wrestling match with all the correographed moves. If it wasn't so rediculous and disappointing, it would be hilarious. In Fact; audience was actually laughing at times. Legendary; you can't seem to do justice to these iconic charactors because the people you use to write, direct and produce don't understand them, so please stop trying. Dan Stevens: you have done great work in the past, please don't sell out to this garbage.\n",
            "The sentiment of the review is negative.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load IMDb dataset\n",
        "# Replace 'imdb_dataset.csv' with the path to your IMDb dataset\n",
        "imdb_data = pd.read_csv('IMDB-Dataset.csv')\n",
        "\n",
        "# Preprocess dataset\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    # Remove punctuation and non-alphabetic characters\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    # Remove stopwords\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    # Lemmatize\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "imdb_data['review'] = imdb_data['review'].apply(preprocess_text)\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(imdb_data['review'], imdb_data['sentiment'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorize text data\n",
        "vectorizer = TfidfVectorizer(max_features=5000)  # Limit features to top 5000\n",
        "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "X_test_vectorized = vectorizer.transform(X_test)\n",
        "\n",
        "# Train logistic regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train_vectorized, y_train)\n",
        "\n",
        "# Predict sentiment for user input\n",
        "def predict_sentiment(review):\n",
        "    preprocessed_review = preprocess_text(review)\n",
        "    review_vectorized = vectorizer.transform([preprocessed_review])\n",
        "    prediction = model.predict(review_vectorized)\n",
        "    return prediction[0]\n",
        "\n",
        "# Get user input\n",
        "user_review = input(\"Enter your movie review: \")\n",
        "\n",
        "# Predict sentiment\n",
        "sentiment = predict_sentiment(user_review)\n",
        "\n",
        "# Print result\n",
        "if sentiment == 'positive':\n",
        "    print(\"The sentiment of the review is positive.\")\n",
        "else:\n",
        "    print(\"The sentiment of the review is negative.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense\n",
        "from keras.utils import to_categorical\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load IMDb dataset\n",
        "# Replace 'imdb_dataset.csv' with the path to your IMDb dataset\n",
        "imdb_data = pd.read_csv('IMDB-Dataset.csv')\n",
        "\n",
        "# Preprocess dataset\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    # Remove punctuation and non-alphabetic characters\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    # Remove stopwords\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    # Lemmatize\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "imdb_data['review'] = imdb_data['review'].apply(preprocess_text)\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "imdb_data['sentiment'] = label_encoder.fit_transform(imdb_data['sentiment'])\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(imdb_data['review'], imdb_data['sentiment'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenize text data\n",
        "max_words = 10000\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "max_len = 200  # Limiting sequence length to 200 words\n",
        "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_len)\n",
        "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_len)\n",
        "\n",
        "# Define CNN model\n",
        "embedding_dim = 100\n",
        "num_filters = 128\n",
        "kernel_size = 5\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length=max_len))\n",
        "model.add(Conv1D(num_filters, kernel_size, activation='relu'))\n",
        "model.add(MaxPooling1D(2))\n",
        "model.add(Conv1D(num_filters, kernel_size, activation='relu'))\n",
        "model.add(MaxPooling1D(2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train_padded, y_train, epochs=5, batch_size=64, validation_data=(X_test_padded, y_test))\n",
        "\n",
        "# Evaluate model\n",
        "loss, accuracy = model.evaluate(X_test_padded, y_test)\n",
        "print(\"Test Accuracy:\", accuracy)\n",
        "\n",
        "# Predict sentiment for user input\n",
        "def predict_sentiment(review):\n",
        "    preprocessed_review = preprocess_text(review)\n",
        "    review_sequence = tokenizer.texts_to_sequences([preprocessed_review])\n",
        "    review_padded = pad_sequences(review_sequence, maxlen=max_len)\n",
        "    prediction = model.predict(review_padded)\n",
        "    return prediction[0][0]\n",
        "\n",
        "# Get user input\n",
        "user_review = input(\"Enter your movie review: \")\n",
        "\n",
        "# Predict sentiment\n",
        "sentiment_score = predict_sentiment(user_review)\n",
        "\n",
        "# Print result\n",
        "if sentiment_score >= 0.5:\n",
        "    print(\"The sentiment of the review is positive.\")\n",
        "else:\n",
        "    print(\"The sentiment of the review is negative.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0CbF3ud6gdP",
        "outputId": "f7c039ed-3b57-442e-fac9-bd509c58c119"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "625/625 [==============================] - 168s 267ms/step - loss: 0.3545 - accuracy: 0.8329 - val_loss: 0.2536 - val_accuracy: 0.8942\n",
            "Epoch 2/5\n",
            "625/625 [==============================] - 172s 276ms/step - loss: 0.1968 - accuracy: 0.9242 - val_loss: 0.2643 - val_accuracy: 0.8899\n",
            "Epoch 3/5\n",
            "625/625 [==============================] - 164s 263ms/step - loss: 0.1205 - accuracy: 0.9561 - val_loss: 0.3575 - val_accuracy: 0.8754\n",
            "Epoch 4/5\n",
            "625/625 [==============================] - 163s 261ms/step - loss: 0.0558 - accuracy: 0.9803 - val_loss: 0.4720 - val_accuracy: 0.8780\n",
            "Epoch 5/5\n",
            "625/625 [==============================] - 159s 254ms/step - loss: 0.0271 - accuracy: 0.9905 - val_loss: 0.6883 - val_accuracy: 0.8744\n",
            "313/313 [==============================] - 11s 35ms/step - loss: 0.6883 - accuracy: 0.8744\n",
            "Test Accuracy: 0.8744000196456909\n",
            "Enter your movie review: I was really excited for Godzilla vs. Kong, but it let me down. The story felt thin, and I just couldn't get attached to any of the characters. It seemed like all the effort went into the special effects, (and even the cgi sucked) and not enough into the story that makes you care. Sometimes, the fights were so over the top that I couldn't even keep up, and instead of being fun, it was just confusing. I wanted to love it for the spectacle, but without a story to hook me, I walked away feeling pretty disappointed. I don't recommend wasting your time watching this garbage, just walk away or watch anything else.\n",
            "1/1 [==============================] - 0s 132ms/step\n",
            "The sentiment of the review is negative.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dbSMb-it-eKq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}